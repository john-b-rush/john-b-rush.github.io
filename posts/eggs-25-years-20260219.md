---
title: "Codex, Claude and 25 Years of Eggs"
description: "I've been scanning grocery receipts since college, waiting for computer vision to catch up. Two AI agents, 10,925 receipts, and 14 days later — did we finally make it?"
---

# Codex, Claude and 25 Years of Eggs

*Published: February 19, 2026*

I've been scanning grocery receipts since 2001. Twenty-five years, three cities, thousands of scans. The thesis was always simple: computer vision will eventually be able to read all of these, and when it does, the data will be interesting. I never typed in a single price. I just kept scanning.

This year I tested the question. Two AI coding agents — OpenAI's Codex CLI and Anthropic's Claude Code — 10,925 receipts, and eggs as the lens. Everyone buys eggs. The price is on every receipt. And if you can track one item across 25 years of garbled thermal prints, OCR failures, and folder typos, you can track anything.

14 days. ~$600 in API costs. 595 egg receipts found. Here's what the data says.

## 25 Years of Egg Prices

<div id="egg-price-chart"></div>

$0.58/dozen at WinCo in 2002 — rural Idaho pricing. $5.78/dozen average in 2025. Try "Inflation-Adjusted" — eggs were actually getting *cheaper* in real terms until the 2022 avian flu spike.

## Where I Bought Them

<div id="egg-map"></div>

## The Running Total

<div id="egg-counter"></div>

## Personal Egg-flation

<div id="egg-inflation"></div>

## Every Purchase, Every Week

<div id="egg-heatmap"></div>

---

## How We Got Here

Here's what I typed on Day 1:

> Ok so let's make a project plan. In the ~/Records/ we have a ton of receipts. Many are pdf/image/etc. I want to go through and extract the actual content of the receipts to find how much we spend on eggs. Receipts are notoriously terrible to OCR, so we might need to do something more advanced.

That was the entire spec. Codex explored my file system, found two existing SQLite databases I'd forgotten about, discovered 5,303 receipts across PDFs, emails, and images, and came back with a project plan. I said "write this out to a plan.md please." It did. We were building within the hour.

The project spanned 14 days. Log analysis shows about 10-14 hours of actual hands-on time — short bursts of direction-giving separated by long stretches of autonomous execution. Codex ran 15 interactive sessions. Claude handled 10. Between them, they spawned 16,762 automated pipeline runs.

<img src="../assets/eggs/eggs-pipeline.svg" alt="Receipt processing pipeline diagram" style="width:100%;margin:1.5em 0">

### Shades of White

The oldest receipts were flatbed scans — multiple receipts per page, random orientations, white paper on a white scanner bed. Codex and I tried seven classical CV approaches to find receipt boundaries. Edge detection. Adaptive thresholding. Contour analysis. Morphological operations. Watershed segmentation. Template matching. A grid-based decomposition I pitched as "a classic HackerRank problem."

None of them worked. The core issue: receipts are white and so is the scanner bed. I started calling it the "shades of white" problem. The cleverest attempt was inspired by removing tourists from landmark photos — stack all scans, compute the median pixel at each position, subtract to reveal edges. I thought that one was going to work. Best F1: 0.302.

We also threw macOS Vision OCR at it (via a Swift script Codex wrote on the fly), Tesseract, several other tools. I was starting to think the flatbed scans might just be a loss. Then I tried Meta's SAM3. One API call with `text="receipt"`. 0.92-0.98 confidence on every boundary. Four seconds per scan. 2,163 receipt boxes from 622 multi-receipt pages. Seven approaches in hours; SAM3 in an afternoon. That was the whole argument for foundation models, in four seconds.

### Wait — You Already Know the Answer

Receipts land at random angles, and OCR needs them upright. I tested five rotation detection methods — local VLMs, Tesseract, various Claude models. Then I realized Claude was already correctly identifying rotation every time I showed it a receipt during our conversation.

Why can't we just use you? So we did. Claude Sonnet's vision on all 2,163 receipts.

### The Overnight OCR Revolution

Halfway through the project, Tesseract was the weak link. It read "OAT MILK" as "OATH ILK." It dropped decimals — `$4.37` became `$437`. On old thermal prints it produced nothing at all. Codex opened 20 of the worst ones by hand and found that some weren't even receipts. A family photo. A postcard. A greeting card. All filed under "Receipts."

I found PaddleOCR-VL — a 0.9B parameter vision-language model that runs locally on Apple Silicon. First test on a bank reconciliation: clean, accurate text in 2.1 seconds. Tesseract was faster but dramatically noisier. Second test on a tall Fred Meyer receipt: disaster. The model entered a repetition loop, hallucinating "TILL YGRT" endlessly. That's the weakness of small autoregressive models on long repetitive content.

The fix turned out to be simple — split tall receipts into slices. Dynamic slicing based on aspect ratio: `num_slices = max(2, round(aspect_ratio / 1.5))`. Five parallel shards ran overnight. GPU pegged at 100% for 10.8 hours. In the morning: 8,948 out of 8,953 receipts OCR'd successfully. Cleaner text for every receipt in the archive.

PaddleOCR-VL isn't a Claude replacement — it can't do structured extraction or follow instructions. It's a better Tesseract. The real pipeline: `receipt image → PaddleOCR-VL (local, clean text) → Claude/Codex (structured extraction)`.

### The Pipeline That Kept Dying

Once receipts were segmented, oriented, and OCR'd, they needed structured extraction — find the egg line items, pull prices and quantities. This is where things got messy.

It started with regex. The models love regex. Keyword matching for "egg," money patterns for prices. Heuristics found eggs in 25/25 positive samples with 0 false positives. Not bad. But on the full corpus, false negatives piled up — Fred Meyer abbreviated codes like `STO LRG BRUNN`, Whole Foods truncated to `EDGS`, OCR mangled "EGGS" into `LG EGO 12 CT`. No regex catches these.

So I told Codex "we have unlimited tokens, it's no big deal," and we pivoted to sending everything to a vision model. Codex self-designed a parallel worker architecture — sharding, worker health management, checkpoint-based resumption, retry logic — all from that one sentence of direction. When Claude's CLI login failed mid-run, Codex auto-disabled Claude and continued solo. I didn't ask it to do that.

But the runs kept crashing. Long CLI jobs died when sessions timed out. The script committed results at end-of-run, so early deaths lost everything. I watched it happen three times. On the fourth attempt I said "I would have expected we start a new process per batch." That was the fix — one fresh process per batch, hard call cap, exit cleanly, resume from cache. Codex patched it, launched it in a tmux session, and the ETA dropped from 12 hours to 1.5.

3,819 receipts processed. The system that was projected to take all night finished before I went to bed.

### The Classifier That Beat Its Own Ground Truth

Regex found 637 receipts mentioning "egg." Against 375 hand-labeled examples: 88% recall. The misses told the story — abbreviated codes, OCR garble, truncated descriptions. No keyword search catches `STO LRG BRUNN`.

The fix: use those hand-labeled edge cases as few-shot examples in an LLM classifier. Twenty examples of what "eggs" looks like on a garbled thermal print from 2003. Batch 10 receipts per call. Eight parallel workers. Two hours. 14,070 receipts classified.

Final accuracy: 99%+. And here's the thing — every supposed "miss" by the LLM turned out to be a mislabel in the ground truth. A bicycle shop receipt the old heuristic had flagged. A barcode-only scan. Egg noodles. The classifier was more correct than the labels I was testing it against.

### Quality

I QA'd 372 stratified random samples across three strata — confirmed egg receipts, possible-seller non-eggs, and non-egg-sellers. Overall: 95.97% correct. The errors were mostly garbled OCR on old scans. One was a hallucination — the pipeline fabricated egg data for a receipt that contained no eggs at all.

Real-world data is relentlessly messy:

- Receipt folder typos spanning years: "Reciepts" (2016-2017) and "Recipts"
- A family photo filed under "Receipts"
- A receipt scanned backwards — Claude decoded the mirrored OCR character by character
- Email receipts silently preferring `text/plain` over `text/html`, dropping pricing lines that only existed in the HTML part
- 164,593 rate limit hits from the automated pipeline grinding against the API

That last number isn't a typo. The pipeline hit rate limits 164,593 times over 14 days. It just kept retrying.

The email bug was one I caught manually. I was reviewing receipts and found "WILCOX FARM Omega 3 Large Grade AA Brown Eggs, 18 CT Qty: 1 @ $9.99 each $7.98" sitting right there in the HTML body — but the pipeline had missed it because the email parser was reading `text/plain` instead of `text/html`. Nine missing egg prices recovered in one fix. I also caught a store address hiding in OCR noise: "915 Ny 45th St" was 915 NW 45th St, Seattle. That led to a recovery pass on 40 missing-location receipts — all 40 resolved.

I repeatedly simplified over-engineered proposals. "I don't know why we need to hash the store id, we can just use it." I reviewed 1,235 items across four QA tools the agents built for me. The agents were good at building. They were not good at knowing when to stop building.

### So Did We Make It?

| Metric | Value |
|--------|-------|
| **Wall clock time** | 14 days (Feb 8-22, 2026) |
| **Hands-on time** | ~10-14 hours |
| **Automated pipeline runs** | 16,762 |
| **Tokens consumed** | ~456 million |
| **Estimated API cost** | ~$600 |
| **Confirmed egg receipts** | 595 |
| **Total egg spend captured** | ~$1,734 |
| **Total eggs** | 7,328 |

Mostly. SAM3 solved receipt segmentation in an afternoon. PaddleOCR replaced Tesseract overnight. The LLM classifier hit 99%+ accuracy and was more correct than my own labels. I never typed a single price.

But it still took $600, 14 days, and me catching bugs the agents missed. Runs crashed at 3 AM. A $9.99 egg purchase was invisible until I opened the HTML myself. The agents built four separate QA tools and I had to use all of them. We're close. The "30 years of eggs" post is already in planning.

<link rel="stylesheet" href="../assets/eggs/eggs-styles.css">
<script src="../assets/eggs/eggs-data.js"></script>
<script src="../assets/eggs/eggs-chart.js"></script>
<script src="../assets/eggs/eggs-map.js"></script>
<script src="../assets/eggs/eggs-counter.js"></script>
<script src="../assets/eggs/eggs-inflation.js"></script>
<script src="../assets/eggs/eggs-heatmap.js"></script>
