---
title: "Codex, Claude and 25 Years of Eggs"
description: "A 25-year bet on computer vision, two AI coding agents, and $506 to answer a question I've been saving receipts for since college."
---

# Codex, Claude and 25 Years of Eggs

*Published: February 19, 2026*

Since 2001, I've kept my receipts. What started as a college budget spreadsheet became a hobby — some would say compulsion — of scanning and archiving every grocery receipt I could get my hands on. Twenty-five years, three cities, thousands of scans. The original bet was simple: computer vision would eventually be able to read all of these, and the data would be interesting. This month, I finally tested that bet with two AI coding agents and some foundational models. 503 egg receipts, $1,734 in total egg spend, and 7,328 eggs later — here's what I found.

## 25 Years of Egg Prices

Try "Inflation-Adjusted" — eggs were actually getting *cheaper* until the 2022 avian flu spike.

<div id="egg-price-chart"></div>

## Where I Bought Them

Moscow, Idaho to Portland to Seattle. Hit play.

<div id="egg-map"></div>

## The Running Total

Click to replay.

<div id="egg-counter"></div>

## Every Purchase, Every Week

Twenty-five years of near-weekly egg buying. The gaps are vacations.

<div id="egg-heatmap"></div>

---

## Why This Exists Now

When I started scanning receipts in college, the technology to actually *read* them didn't exist — not reliably, not at scale. But it was obvious that it would. So I kept scanning, kept archiving, and waited. The bet was that someday the extraction would be trivial, and the interesting part would be what the data actually says.

We're not quite at "trivial" yet, but we crossed a threshold this year. What would have taken hundreds of hours of manual data entry — typing in prices, dates, store names from thousands of garbled receipt scans — took about a dozen evenings of pair programming with AI agents. I never typed a single price. I never manually entered a store name. The spreadsheet approach was never realistic for this volume. The AI approach finally was.

Part of the exercise was genuinely testing where foundational models are strong and where they still fall short. I wanted to feel the difference between classical computer vision and modern segmentation models, between hand-tuned regex and LLM extraction, between heuristic pipelines and just throwing everything at a vision model. You can read about these tradeoffs. Until you run 5,000+ receipts through them, you don't really internalize them.

My role was creative direction and QA. I decided the approaches, reviewed the output, caught the bugs. The machines did the tedious execution — and a lot of it without much prompting. I gave Codex a direction like "use SQLite" or "extract egg data from my receipts" and it figured out that with thousands of receipts it needed parallel workers, sharding, checkpoint-based resumption, and retry logic. I deliberately didn't overspecify. How far could the agents get on their own? Pretty far, it turns out — Codex ran a single continuous session for 12 days, spawning 11,729 automated pipeline runs on its own.

There's something genuinely surreal about realizing a 25-year dream and not having to type in a single data point to get there. The "30 years of eggs" post is already in planning.

## How It Was Made

The project spanned 12 days of wall clock time, but log analysis shows about 10-14 hours of actual hands-on time — short bursts of direction-giving separated by long stretches of autonomous AI execution. The median gap between my messages during active periods was 24 seconds. Then I'd walk away for hours while the pipeline ran.

<img src="../assets/eggs/eggs-pipeline.svg" alt="Receipt processing pipeline diagram" style="width:100%;margin:1.5em 0">

### The Scanner Problem

The oldest receipts were on a flatbed scanner — multiple receipts per scan, random orientations, white paper on a white scanner bed. This was a significant barrier. The later receipts were scanned individually on a ScanSnap and were much cleaner, but those early flatbed scans needed real work.

Codex CLI and I tried seven classical CV approaches to find receipt boundaries — edge detection, adaptive thresholding, contour analysis, morphological operations, watershed segmentation, template matching, and a grid-based decomposition I pitched as "a classic HackerRank problem." The core issue: receipts are white and so is the scanner bed. I started calling it the "shades of white" problem. The cleverest attempt was inspired by removing tourists from landmark photos — stack all scans, compute the median pixel at each position, subtract to reveal edges. Best F1: 0.302.

We also tested macOS Vision OCR (via a Swift script Codex wrote on the fly), Tesseract, and several other tools. None of the classical approaches generalized. But the work wasn't wasted — the Flask review app and evaluation workflow carried forward into later stages, and honestly, it was worth doing just to feel the gap between old-school CV and what came next.

What came next: Meta's SAM3. One API call with `text="receipt"` found every boundary with 0.92-0.98 confidence. Piloted on 25 scans, ran the full set, got 2,163 receipt boxes at about 4 seconds per scan. The contrast with hours of hand-tuned thresholding was visceral. You can read about foundation models being better. Running them back-to-back on the same problem makes it real.

Then orientation detection — receipts land at random angles, and OCR needs them upright. I tested five methods, from local VLMs to Tesseract to various Claude models. The winning moment was realizing that Claude was already correctly identifying rotation every time I showed it a receipt during our conversation. Wait — you already know the answer. Why can't we just use you? So we did. Claude Sonnet's vision, directly, on all 2,163 receipts.

### The Pipeline

Once every receipt was individualized and oriented, they all went through the same extraction pipeline — the flatbed crops, the ScanSnap scans, the PDFs, the email receipts. Thousands of images and documents, 2002 through 2026.

It started with heuristic text parsing — regex for money amounts, keyword matching for egg mentions. The models love regex, for whatever reason. But heuristics only get you so far with messy real-world data, and when I said "we have unlimited tokens, it's no big deal," we pivoted to just sending everything to a vision model for structured extraction.

Codex ran a continuous 12-day session, building the pipeline as it went. It self-designed a parallel worker architecture, split receipts into shards, managed worker health, merged outputs, and handled retries — all from fairly minimal direction on my part. When Claude CLI login failed mid-run, the system auto-disabled Claude and continued with Codex only. I didn't ask it to do that.

### What the Pipeline Found

| Year | Receipts | Egg Receipts | Egg Spend | Avg $/doz |
|------|----------|-------------|-----------|-----------|
| 2002 | 172 | 6 | $3.71 | $0.73 |
| 2003 | 300 | 20 | $20.20 | $1.01 |
| 2004 | 280 | 17 | $21.17 | $1.11 |
| 2005 | 310 | 13 | $17.25 | $1.09 |
| 2006 | 400 | 13 | $21.50 | $1.65 |
| 2007 | 402 | 7 | $12.34 | $1.76 |
| 2008 | 258 | 2 | $3.98 | $1.99 |
| 2009 | 239 | 7 | $13.78 | $2.07 |
| 2010 | 359 | 22 | $45.06 | $2.05 |
| 2011 | 213 | 11 | $19.59 | $1.87 |
| 2012 | 80 | 5 | $8.15 | $1.63 |
| 2013 | 520 | 24 | $49.45 | $2.06 |
| 2014 | 473 | 41 | $145.39 | $3.50 |
| 2015 | 1,129 | 34 | $116.70 | $3.15 |
| 2016 | 560 | 11 | $35.98 | $2.77 |
| 2017 | 681 | 9 | $22.52 | $2.50 |
| 2018 | 820 | 39 | $90.97 | $2.12 |
| 2019 | 889 | 52 | $117.81 | $1.96 |
| 2020 | 503 | 21 | $119.20 | $2.21 |
| 2021 | 494 | 44 | $223.04 | $3.98 |
| 2022 | 513 | 29 | $127.78 | $3.76 |
| 2023 | 461 | 39 | $233.67 | $4.16 |
| 2024 | 352 | 23 | $178.58 | $3.50 |
| 2025 | 469 | 12 | $75.19 | $5.78 |
| 2026 | 46 | 2 | $11.28 | $5.64 |

Some things that jump out:

- **2002 WinCo at $0.58/dozen** — eggs were genuinely this cheap in rural Idaho, though OCR might have dropped a digit. WinCo's bulk pricing held under $1/dozen through 2005.
- **2008 has only 2 egg receipts** — I was between scanners and barely archiving. The dip in 2012 (80 total receipts) is the same problem.
- **2014 spike to $3.50/dozen** — first avian flu crisis. Prices didn't fully recover until 2019.
- **2021-2023 sustained highs** — second avian flu wave plus COVID supply chain. $4.16/dozen average in 2023.
- **2025 at $5.78/dozen** — the current crisis peak. The cheapest eggs I found were $2.99.
- **Trader Joe's accounts for 51% of all egg purchases** (256 receipts). WinCo dominated the college years, Whole Foods entered in 2021.
- **Biggest single purchase**: $36.59 at Chef'Store for 15 dozen (180 eggs). Party.
- **An $8.29/dozen receipt from Foodland** — that one's from Hawaii.

Not all the data is clean. 170 of 503 egg receipts had quantities inferred from price and time-period averages rather than explicitly parsed. A few receipts have date-parsing errors (one receipt thinks it's from 1985). The per-egg cost outliers usually trace back to quantity inference problems — a $15.54/dozen "outlier" in 2023 is almost certainly a normal dozen that got classified as 2 eggs.

### The Messiness

Real-world data is relentlessly messy:

- Receipt folder typos spanning years: "Reciepts" (2016-2017) and "Recipts"
- A family photo filed under "Receipts"
- A receipt scanned backwards — Claude decoded the mirrored OCR character by character
- Email receipts silently preferring `text/plain` over `text/html`, dropping the pricing lines that only existed in the HTML part
- 42,000 rate limit hits from the automated pipeline grinding against the API

### Human in the Loop

The AI built the pipeline. Manual review remained essential at every quality gate.

I spotted a store address hiding in OCR noise: "915 Ny 45th St" was 915 NW 45th St, Seattle. The AI had the text but didn't recognize it as a fixable artifact. That led to a Claude recovery pass on 40 missing-location egg receipts — all 40 resolved.

The biggest catch: I manually reviewed 18 receipts and found "WILCOX FARM Omega 3 Large Grade AA Brown Eggs, 18 CT Qty: 1 @ $9.99 each $7.98" sitting right there in the data. "You couldn't figure that out?" This exposed a fundamental bug — the email parser was preferring `text/plain` over the richer `text/html` that contained the actual pricing. Nine missing egg prices recovered in one fix.

I also repeatedly simplified over-engineered proposals. "I don't know why we need to hash the store id, we can just use it." And "I would have expected we start a new process per batch" — which turned out to be the fix for hours of process instability.

### The Numbers

| Metric | Value |
|--------|-------|
| **Wall clock time** | 12 days (Feb 8-20, 2026) |
| **Hands-on time** | ~10-14 hours |
| **Automated pipeline runs** | 11,729 |
| **Tokens consumed** | ~409 million |
| **Estimated API cost** | ~$506 |
| **Confirmed egg receipts** | 503 |
| **Total egg spend captured** | ~$1,734 |
| **Rate limit hits** | 42,000 |

The project log itself is part of the project — Claude Opus narrating what Codex built, from logs generated by both systems. The narrative was written by 6 parallel Sonnet subagents, each reading pre-condensed transcripts. AI parallelism writing about AI parallelism.

The visualizations are vanilla JavaScript — no libraries, just Canvas and SVG. Code is [on GitHub](https://github.com/john-b-rush/john-b-rush.github.io).

<link rel="stylesheet" href="../assets/eggs/eggs-styles.css">
<script src="../assets/eggs/eggs-data.js"></script>
<script src="../assets/eggs/eggs-chart.js"></script>
<script src="../assets/eggs/eggs-map.js"></script>
<script src="../assets/eggs/eggs-counter.js"></script>
<script src="../assets/eggs/eggs-heatmap.js"></script>
