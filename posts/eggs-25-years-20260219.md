---
title: "Codex, Claude and 25 Years of Eggs"
description: "I've been scanning grocery receipts since college, waiting for computer vision to catch up. Two AI agents, 10,925 receipts, and 14 days later — did we finally make it?"
---

# Codex, Claude and 25 Years of Eggs

*Published: February 19, 2026*

I've been scanning grocery receipts since 2001. Twenty-five years, three cities, thousands of scans. The thesis was always simple: computer vision will eventually be able to read all of these, and when it does, the data will be interesting. I never typed in a single price. I just kept scanning.

This year I tested the question. Two AI coding agents — OpenAI's Codex CLI and Anthropic's Claude Code — 10,925 receipts, and eggs as the lens. Everyone buys eggs. The price is on every receipt. And if you can track one item across 25 years of garbled thermal prints, OCR failures, and folder typos, you can track anything.

14 days. ~$600 in API costs. 595 egg receipts found. Here's what the data says.

## 25 Years of Egg Prices

<div id="egg-price-chart"></div>

$0.58/dozen at WinCo in 2002 — rural Idaho pricing. $5.78/dozen average in 2025. The thing is, eggs were actually getting *cheaper* in real terms until the 2022 avian flu spike.

## Where I Bought Them

<div id="egg-map"></div>

## The Running Total

<div id="egg-counter"></div>

## Personal Egg-flation

<div id="egg-inflation"></div>

## Every Purchase, Every Week

<div id="egg-heatmap"></div>

---

## How We Got Here

The project spanned 14 days of wall clock time, but log analysis shows about 10-14 hours of actual hands-on time — short bursts of direction-giving separated by long stretches of autonomous AI execution. I'd give a direction, review the output, walk away for hours while the pipeline ran.

My role was creative direction and QA. The agents did the execution. Codex ran 15 interactive sessions over 14 continuous days. Claude handled 10 sessions of targeted extraction, QA tooling, and the final classification pass. Between them, they spawned 16,762 automated pipeline runs.

<img src="../assets/eggs/eggs-pipeline.svg" alt="Receipt processing pipeline diagram" style="width:100%;margin:1.5em 0">

### Shades of White

The oldest receipts were flatbed scans — multiple receipts per page, random orientations, white paper on a white scanner bed. Codex and I tried seven classical CV approaches to find receipt boundaries. Edge detection, adaptive thresholding, contour analysis, morphological operations, watershed segmentation, template matching, and a grid-based decomposition I pitched as "a classic HackerRank problem."

The core issue: receipts are white and so is the scanner bed. The cleverest attempt was inspired by removing tourists from landmark photos — stack all scans, compute the median pixel at each position, subtract to reveal edges. Best F1: 0.302.

Then Meta's SAM3. One API call with `text="receipt"` found every boundary with 0.92-0.98 confidence. Four seconds per scan. 2,163 receipt boxes from 622 multi-receipt pages. The contrast with hours of hand-tuned thresholding was visceral. You can read about foundation models being better. Running them back-to-back on the same problem makes it real.

### Wait — You Already Know the Answer

Receipts land at random angles, and OCR needs them upright. I tested five rotation detection methods, from local VLMs to Tesseract to various Claude models. The winning moment was realizing that Claude was already correctly identifying rotation every time I showed it a receipt during our conversation.

Why can't we just use you? So we did. Claude Sonnet's vision on all 2,163 receipts.

### The PaddleOCR Revolution

Halfway through, Tesseract was the bottleneck. Garbled output, especially on old thermal prints. I found PaddleOCR-VL — a 0.9B parameter vision-language model that runs locally on Apple Silicon.

First test on a bank reconciliation: clean, accurate text in 2.1 seconds. Tesseract was faster but dramatically noisier. Second test on a tall receipt: disaster. The model entered a repetition loop, hallucinating the same line item endlessly. The fix was simple — split tall receipts into slices. Dynamic slicing based on aspect ratio: `num_slices = max(2, round(aspect_ratio / 1.5))`.

Five parallel shards ran overnight. 8,948 out of 8,953 receipts OCR'd successfully. 10.8 hours of wall time, GPU pegged the entire run. In the morning I had cleaner text for every receipt in the archive.

PaddleOCR-VL isn't a Claude replacement — it can't do structured extraction. It's a better Tesseract. The pipeline: `receipt image → PaddleOCR-VL (local, clean text) → Claude/Codex (structured extraction)`.

### The Pipeline

Once every receipt was individualized, oriented, and OCR'd, they all went through the same extraction pipeline. The flatbed crops, the ScanSnap scans, the PDFs, the email receipts. Thousands of documents, 2002 through 2026.

It started with regex for money amounts and keyword matching for egg mentions. The models love regex. But heuristics only get you so far with messy real-world data, and when I said "we have unlimited tokens, it's no big deal," we pivoted to sending everything to a vision model for structured extraction.

Codex self-designed a parallel worker architecture, split receipts into shards, managed worker health, merged outputs, and handled retries — all from fairly minimal direction. When Claude CLI login failed mid-run, the system auto-disabled Claude and continued with Codex only. I didn't ask it to do that.

### The Classifier

Regex found 637 receipts mentioning "egg." Against 375 hand-labeled ground truth: 88% recall, 94% precision. The false negatives tell the story: Fred Meyer abbreviated codes (`STO LRG BRUNN`), Whole Foods truncated (`EDGS`), OCR manglings (`LG EGO 12 CT`). No regex can catch these.

The fix: use the hand-labeled edge cases as few-shot examples in an LLM classifier. Batch 10 receipts per call, include ~20 examples of what "eggs" looks like on a garbled thermal print from 2003. Eight parallel workers, ~250 receipts/minute. Two hours.

Final accuracy: 99%+. The LLM was actually more correct than the "ground truth" labels. Every supposed "miss" turned out to be a mislabel from the old heuristic pipeline — a bicycle shop receipt, a barcode-only scan, egg noodles.

### Quality

I QA'd 372 stratified random samples. Overall: 95.97% correct. The 14 errors in egg receipts were mostly garbled OCR and one hallucination — the pipeline fabricated egg data for a receipt that contained no eggs at all.

The thing is, real-world data is relentlessly messy:

- Receipt folder typos spanning years: "Reciepts" (2016-2017) and "Recipts"
- A family photo filed under "Receipts"
- A receipt scanned backwards — Claude decoded the mirrored OCR character by character
- Email receipts silently preferring `text/plain` over `text/html`, dropping pricing lines that only existed in the HTML part
- 164,593 rate limit hits from the automated pipeline grinding against the API

### The Human Parts

The AI built the pipeline. Manual review remained essential at every quality gate.

I spotted a store address hiding in OCR noise: "915 Ny 45th St" was 915 NW 45th St, Seattle. That led to a recovery pass on 40 missing-location receipts — all 40 resolved. I found "WILCOX FARM Omega 3 Large Grade AA Brown Eggs, 18 CT Qty: 1 @ $9.99 each $7.98" sitting right there in the data that the pipeline missed. That exposed a fundamental bug — the email parser was preferring `text/plain` over the richer `text/html`. Nine missing egg prices recovered in one fix.

I also repeatedly simplified over-engineered proposals. "I don't know why we need to hash the store id, we can just use it." And "I would have expected we start a new process per batch" — which turned out to be the fix for hours of process instability.

I reviewed 1,235 items across four QA tools the agents built for me. The agents proposed. I decided. The agents executed. I verified.

### So Did We Make It?

| Metric | Value |
|--------|-------|
| **Wall clock time** | 14 days (Feb 8-22, 2026) |
| **Hands-on time** | ~10-14 hours |
| **Automated pipeline runs** | 16,762 |
| **Tokens consumed** | ~456 million |
| **Estimated API cost** | ~$600 |
| **Confirmed egg receipts** | 595 |
| **Total egg spend captured** | ~$1,734 |
| **Total eggs** | 7,328 |

Mostly. SAM3 crushed the segmentation problem. PaddleOCR replaced Tesseract overnight. The LLM classifier hit 99%+ accuracy. I never typed a single price. But it still took $600, 14 days, human QA at every gate, and me catching bugs the agents missed. We're at the threshold, not past it.

The "30 years of eggs" post is already in planning.

<link rel="stylesheet" href="../assets/eggs/eggs-styles.css">
<script src="../assets/eggs/eggs-data.js"></script>
<script src="../assets/eggs/eggs-chart.js"></script>
<script src="../assets/eggs/eggs-map.js"></script>
<script src="../assets/eggs/eggs-counter.js"></script>
<script src="../assets/eggs/eggs-inflation.js"></script>
<script src="../assets/eggs/eggs-heatmap.js"></script>
